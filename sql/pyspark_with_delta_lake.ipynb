{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T06:37:52.633624Z",
     "start_time": "2025-06-20T06:37:52.629513Z"
    }
   },
   "cell_type": "code",
   "source": "from delta_lake_spark_utils import sparksession_with_delta_lake",
   "id": "6d0287a568d31bd9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T06:37:55.291864Z",
     "start_time": "2025-06-20T06:37:54.179854Z"
    }
   },
   "cell_type": "code",
   "source": "spark = sparksession_with_delta_lake()",
   "id": "826dc7b564f428e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created successfully with Delta Lake support!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a131c3d916bf7e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing data to Delta Lake table at: ./delta_tables/my_delta_table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written successfully!\n",
      "\n",
      "Reading data from Delta Lake table at: ./delta_tables/my_delta_table\n",
      "+-------+---+--------+\n",
      "|   Name| ID|    City|\n",
      "+-------+---+--------+\n",
      "|  Alice|  1|New York|\n",
      "|Charlie|  3|   Paris|\n",
      "|    Bob|  2|  London|\n",
      "+-------+---+--------+\n",
      "\n",
      "\n",
      "Updating Bob's city...\n",
      "Data after update:\n",
      "+-------+---+--------+\n",
      "|   Name| ID|    City|\n",
      "+-------+---+--------+\n",
      "|  Alice|  1|New York|\n",
      "|Charlie|  3|   Paris|\n",
      "|    Bob|  2|  Berlin|\n",
      "+-------+---+--------+\n",
      "\n",
      "\n",
      "Appending new data (Eve)...\n",
      "Data after append:\n",
      "+-------+---+--------+\n",
      "|   Name| ID|    City|\n",
      "+-------+---+--------+\n",
      "|  Alice|  1|New York|\n",
      "|Charlie|  3|   Paris|\n",
      "|    Bob|  2|  Berlin|\n",
      "|    Eve|  4|    Rome|\n",
      "+-------+---+--------+\n",
      "\n",
      "\n",
      "Time traveling to version 0 (initial write):\n",
      "+-------+---+--------+\n",
      "|   Name| ID|    City|\n",
      "+-------+---+--------+\n",
      "|  Alice|  1|New York|\n",
      "|Charlie|  3|   Paris|\n",
      "|    Bob|  2|  London|\n",
      "+-------+---+--------+\n",
      "\n",
      "\n",
      "SparkSession stopped.\n"
     ]
    }
   ],
   "execution_count": 6,
   "source": [
    "# 2. Create some sample data\n",
    "data = [(\"Alice\", 1, \"New York\"),\n",
    "        (\"Bob\", 2, \"London\"),\n",
    "        (\"Charlie\", 3, \"Paris\")]\n",
    "columns = [\"Name\", \"ID\", \"City\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# 3. Define a path for your Delta Lake table\n",
    "delta_table_path = \"./delta_tables/my_delta_table\"\n",
    "\n",
    "# 4. Write DataFrame to Delta Lake table\n",
    "print(f\"\\nWriting data to Delta Lake table at: {delta_table_path}\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "print(\"Data written successfully!\")\n",
    "\n",
    "# 5. Read data from Delta Lake table\n",
    "print(f\"\\nReading data from Delta Lake table at: {delta_table_path}\")\n",
    "delta_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "delta_df.show()\n",
    "\n",
    "# 6. Perform an update (example: change Bob's city)\n",
    "#    Delta Lake allows for ACID transactions directly on the table\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "print(\"\\nUpdating Bob's city...\")\n",
    "deltaTable = DeltaTable.forPath(spark, delta_table_path)\n",
    "deltaTable.update(\n",
    "    col(\"Name\") == \"Bob\",\n",
    "    {\"City\": \"'Berlin'\"}\n",
    ")\n",
    "\n",
    "print(\"Data after update:\")\n",
    "spark.read.format(\"delta\").load(delta_table_path).show()\n",
    "\n",
    "# 7. Add new data (example: append Eve)\n",
    "print(\"\\nAppending new data (Eve)...\")\n",
    "new_data = [(\"Eve\", 4, \"Rome\")]\n",
    "new_df = spark.createDataFrame(new_data, columns)\n",
    "new_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
    "\n",
    "print(\"Data after append:\")\n",
    "spark.read.format(\"delta\").load(delta_table_path).show()\n",
    "\n",
    "\n",
    "# 8. Time Travel (read previous version)\n",
    "#    You can specify a version or a timestamp\n",
    "print(\"\\nTime traveling to version 0 (initial write):\")\n",
    "old_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
    "old_df.show()\n",
    "\n",
    "# 9. Stop SparkSession\n",
    "spark.stop()\n",
    "print(\"\\nSparkSession stopped.\")"
   ],
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
